{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fhsu4976/CS598/blob/main/CS598_finalproject.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Colab Notebook for Query-Focused EHR Extractive Summarization\n",
        "# =============================================================\n",
        "# This notebook demonstrates an end-to-end pipeline using a small subset of preprocessed\n",
        "# MIMIC-III .data files to reproduce key experiments from:\n",
        "# \"Query-Focused Extractive Summarization of Electronic Health Records\" (ArXiv:2004.04645).\n"
      ],
      "metadata": {
        "id": "NsVKk4unZkGj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ## 1. Setup\n",
        "# Mount Google Drive, clone the official repo, install dependencies, and download spaCy model.\n",
        "\n",
        "from google.colab import drive\n",
        "import os, sys\n",
        "\n",
        "\n",
        "# Force remount Google Drive to ensure fresh mount\n",
        "drive.mount('/content/drive', force_remount=True)  # Force remount to avoid mountpoint errors\n",
        "\n",
        "# Debug: List top-level of mounted Drive to verify folders\n",
        "print(\"/content/drive contents:\", os.listdir('/content/drive'))\n",
        "print(\"/content/drive/MyDrive contents:\", os.listdir('/content/drive/MyDrive'))\n",
        "\n",
        "# Change to working directory\n",
        "os.chdir('/content')\n",
        "\n",
        "# Clone repository\n",
        "target_repo = '/content/ehr-extraction-models'\n",
        "!rm -rf {target_repo}\n",
        "!git clone https://github.com/dmcinerney/ehr-extraction-models.git {target_repo}\n",
        "\n",
        "# Replace the default requirements.txt with your custom one from Drive\n",
        "# (assumes custom file at /content/drive/MyDrive/CS598/DLH/requirements.txt)\n",
        "custom_req = '/content/drive/MyDrive/CS598/DLH/requirements.txt'\n",
        "!cp {custom_req} {target_repo}/requirements.txt\n",
        "\n",
        "# Install dependencies\n",
        "!pip install -r {target_repo}/requirements.txt\n",
        "!pip install -e git+https://github.com/dmcinerney/pytt.git@4a15322f696fe85a264dd4854fcdb82c9e801c06#egg=pytt\n",
        "\n",
        "# Download spaCy English model\n",
        "!python -m spacy download en_core_web_sm\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "edBIarChZsLe",
        "outputId": "0c9b53f2-d322-4d5b-c1f9-c98613dc2f92"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive contents: ['MyDrive', '.shortcut-targets-by-id', '.file-revisions-by-id', 'Shareddrives', '.Trash-0']\n",
            "/content/drive/MyDrive contents: ['Colab Notebooks', 'Chat history for CS598.txt', 'IMG_3594.png', 'CS598']\n",
            "Cloning into '/content/ehr-extraction-models'...\n",
            "remote: Enumerating objects: 844, done.\u001b[K\n",
            "remote: Counting objects: 100% (124/124), done.\u001b[K\n",
            "remote: Compressing objects: 100% (64/64), done.\u001b[K\n",
            "remote: Total 844 (delta 68), reused 111 (delta 60), pack-reused 720 (from 1)\u001b[K\n",
            "Receiving objects: 100% (844/844), 3.00 MiB | 20.31 MiB/s, done.\n",
            "Resolving deltas: 100% (492/492), done.\n",
            "Collecting setuptools==68.0.0 (from -r /content/ehr-extraction-models/requirements.txt (line 1))\n",
            "  Downloading setuptools-68.0.0-py3-none-any.whl.metadata (6.4 kB)\n",
            "Collecting asgiref==3.2.3 (from -r /content/ehr-extraction-models/requirements.txt (line 3))\n",
            "  Downloading asgiref-3.2.3-py2.py3-none-any.whl.metadata (8.2 kB)\n",
            "Collecting boto3==1.11.15 (from -r /content/ehr-extraction-models/requirements.txt (line 5))\n",
            "  Downloading boto3-1.11.15-py2.py3-none-any.whl.metadata (4.7 kB)\n",
            "Collecting botocore==1.14.15 (from -r /content/ehr-extraction-models/requirements.txt (line 6))\n",
            "  Downloading botocore-1.14.15-py2.py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting cachetools==4.0.0 (from -r /content/ehr-extraction-models/requirements.txt (line 7))\n",
            "  Downloading cachetools-4.0.0-py3-none-any.whl.metadata (4.4 kB)\n",
            "Collecting catalogue==1.0.0 (from -r /content/ehr-extraction-models/requirements.txt (line 8))\n",
            "  Downloading catalogue-1.0.0-py2.py3-none-any.whl.metadata (13 kB)\n",
            "Collecting certifi==2019.11.28 (from -r /content/ehr-extraction-models/requirements.txt (line 9))\n",
            "  Downloading certifi-2019.11.28-py2.py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting chardet==3.0.4 (from -r /content/ehr-extraction-models/requirements.txt (line 10))\n",
            "  Downloading chardet-3.0.4-py2.py3-none-any.whl.metadata (3.2 kB)\n",
            "Collecting Click==7.0 (from -r /content/ehr-extraction-models/requirements.txt (line 11))\n",
            "  Downloading Click-7.0-py2.py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting cycler==0.10.0 (from -r /content/ehr-extraction-models/requirements.txt (line 12))\n",
            "  Downloading cycler-0.10.0-py2.py3-none-any.whl.metadata (722 bytes)\n",
            "Collecting decorator==4.4.1 (from -r /content/ehr-extraction-models/requirements.txt (line 14))\n",
            "  Downloading decorator-4.4.1-py2.py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting Django==3.0.3 (from -r /content/ehr-extraction-models/requirements.txt (line 15))\n",
            "  Downloading Django-3.0.3-py3-none-any.whl.metadata (3.6 kB)\n",
            "Collecting docutils==0.15.2 (from -r /content/ehr-extraction-models/requirements.txt (line 16))\n",
            "  Downloading docutils-0.15.2-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting filelock==3.0.12 (from -r /content/ehr-extraction-models/requirements.txt (line 17))\n",
            "  Downloading filelock-3.0.12-py3-none-any.whl.metadata (4.3 kB)\n",
            "Collecting google-auth==1.11.0 (from -r /content/ehr-extraction-models/requirements.txt (line 18))\n",
            "  Downloading google_auth-1.11.0-py2.py3-none-any.whl.metadata (3.1 kB)\n",
            "Collecting google-auth-oauthlib==0.4.1 (from -r /content/ehr-extraction-models/requirements.txt (line 19))\n",
            "  Downloading google_auth_oauthlib-0.4.1-py2.py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting idna==2.8 (from -r /content/ehr-extraction-models/requirements.txt (line 21))\n",
            "  Downloading idna-2.8-py2.py3-none-any.whl.metadata (8.9 kB)\n",
            "Collecting image==1.5.28 (from -r /content/ehr-extraction-models/requirements.txt (line 22))\n",
            "  Downloading image-1.5.28.tar.gz (15 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting importlib-metadata==1.5.0 (from -r /content/ehr-extraction-models/requirements.txt (line 23))\n",
            "  Downloading importlib_metadata-1.5.0-py2.py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting jmespath==0.9.4 (from -r /content/ehr-extraction-models/requirements.txt (line 24))\n",
            "  Downloading jmespath-0.9.4-py2.py3-none-any.whl.metadata (7.9 kB)\n",
            "Collecting joblib==0.14.1 (from -r /content/ehr-extraction-models/requirements.txt (line 25))\n",
            "  Downloading joblib-0.14.1-py2.py3-none-any.whl.metadata (4.5 kB)\n",
            "Collecting Markdown==3.2 (from -r /content/ehr-extraction-models/requirements.txt (line 27))\n",
            "  Downloading Markdown-3.2-py2.py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting matplotlib==3.7.2 (from -r /content/ehr-extraction-models/requirements.txt (line 28))\n",
            "  Downloading matplotlib-3.7.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
            "Collecting networkx==2.4 (from -r /content/ehr-extraction-models/requirements.txt (line 30))\n",
            "  Downloading networkx-2.4-py3-none-any.whl.metadata (4.9 kB)\n",
            "Collecting numpy==1.22.4 (from -r /content/ehr-extraction-models/requirements.txt (line 31))\n",
            "  Downloading numpy-1.22.4.zip (11.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.5/11.5 MB\u001b[0m \u001b[31m84.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting oauthlib==3.1.0 (from -r /content/ehr-extraction-models/requirements.txt (line 32))\n",
            "  Downloading oauthlib-3.1.0-py2.py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting plac==1.1.3 (from -r /content/ehr-extraction-models/requirements.txt (line 35))\n",
            "  Downloading plac-1.1.3-py2.py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: preshed==3.0.9 in /usr/local/lib/python3.11/dist-packages (from -r /content/ehr-extraction-models/requirements.txt (line 36)) (3.0.9)\n",
            "Collecting protobuf==3.11.3 (from -r /content/ehr-extraction-models/requirements.txt (line 37))\n",
            "  Downloading protobuf-3.11.3-py2.py3-none-any.whl.metadata (884 bytes)\n",
            "Collecting pyasn1==0.4.8 (from -r /content/ehr-extraction-models/requirements.txt (line 38))\n",
            "  Downloading pyasn1-0.4.8-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting pyasn1-modules==0.2.8 (from -r /content/ehr-extraction-models/requirements.txt (line 39))\n",
            "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting pyparsing==2.4.6 (from -r /content/ehr-extraction-models/requirements.txt (line 40))\n",
            "  Downloading pyparsing-2.4.6-py2.py3-none-any.whl.metadata (3.6 kB)\n",
            "Collecting python-dateutil==2.8.1 (from -r /content/ehr-extraction-models/requirements.txt (line 41))\n",
            "  Downloading python_dateutil-2.8.1-py2.py3-none-any.whl.metadata (8.0 kB)\n",
            "Collecting pytz==2019.3 (from -r /content/ehr-extraction-models/requirements.txt (line 42))\n",
            "  Downloading pytz-2019.3-py2.py3-none-any.whl.metadata (20 kB)\n",
            "Collecting regex==2020.1.8 (from -r /content/ehr-extraction-models/requirements.txt (line 43))\n",
            "  Downloading regex-2020.1.8.tar.gz (681 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m681.1/681.1 kB\u001b[0m \u001b[31m32.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting requests==2.22.0 (from -r /content/ehr-extraction-models/requirements.txt (line 44))\n",
            "  Downloading requests-2.22.0-py2.py3-none-any.whl.metadata (5.5 kB)\n",
            "Collecting requests-oauthlib==1.3.0 (from -r /content/ehr-extraction-models/requirements.txt (line 45))\n",
            "  Downloading requests_oauthlib-1.3.0-py2.py3-none-any.whl.metadata (9.6 kB)\n",
            "Collecting rsa==4.0 (from -r /content/ehr-extraction-models/requirements.txt (line 46))\n",
            "  Downloading rsa-4.0-py2.py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting s3transfer==0.3.3 (from -r /content/ehr-extraction-models/requirements.txt (line 47))\n",
            "  Downloading s3transfer-0.3.3-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting sacremoses==0.0.38 (from -r /content/ehr-extraction-models/requirements.txt (line 48))\n",
            "  Downloading sacremoses-0.0.38.tar.gz (860 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m860.8/860.8 kB\u001b[0m \u001b[31m43.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting scipy==1.13.0 (from -r /content/ehr-extraction-models/requirements.txt (line 50))\n",
            "  Downloading scipy-1.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sentencepiece==0.1.99 (from -r /content/ehr-extraction-models/requirements.txt (line 51))\n",
            "  Downloading sentencepiece-0.1.99-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
            "Collecting six==1.14.0 (from -r /content/ehr-extraction-models/requirements.txt (line 52))\n",
            "  Downloading six-1.14.0-py2.py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting sklearn==0.0 (from -r /content/ehr-extraction-models/requirements.txt (line 53))\n",
            "  Downloading sklearn-0.0.tar.gz (1.1 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting spacy==2.3.9 (from -r /content/ehr-extraction-models/requirements.txt (line 54))\n",
            "  Downloading spacy-2.3.9-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (15 kB)\n",
            "Collecting sqlparse==0.3.0 (from -r /content/ehr-extraction-models/requirements.txt (line 55))\n",
            "  Downloading sqlparse-0.3.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting srsly==1.0.7 (from -r /content/ehr-extraction-models/requirements.txt (line 56))\n",
            "  Downloading srsly-1.0.7-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Collecting tensorboard==2.1.0 (from -r /content/ehr-extraction-models/requirements.txt (line 57))\n",
            "  Downloading tensorboard-2.1.0-py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting thinc==7.4.6 (from -r /content/ehr-extraction-models/requirements.txt (line 58))\n",
            "  Downloading thinc-7.4.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (23 kB)\n",
            "Collecting tokenizers==0.13.3 (from -r /content/ehr-extraction-models/requirements.txt (line 59))\n",
            "  Downloading tokenizers-0.13.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Collecting torch==2.0.0 (from -r /content/ehr-extraction-models/requirements.txt (line 60))\n",
            "  Downloading torch-2.0.0-cp311-cp311-manylinux1_x86_64.whl.metadata (24 kB)\n",
            "Collecting tqdm==4.42.1 (from -r /content/ehr-extraction-models/requirements.txt (line 61))\n",
            "  Downloading tqdm-4.42.1-py2.py3-none-any.whl.metadata (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.7/49.7 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting transformers==4.30.0 (from -r /content/ehr-extraction-models/requirements.txt (line 62))\n",
            "  Downloading transformers-4.30.0-py3-none-any.whl.metadata (113 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m113.6/113.6 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting urllib3==1.25.8 (from -r /content/ehr-extraction-models/requirements.txt (line 63))\n",
            "  Downloading urllib3-1.25.8-py2.py3-none-any.whl.metadata (38 kB)\n",
            "Collecting wasabi==0.6.0 (from -r /content/ehr-extraction-models/requirements.txt (line 64))\n",
            "  Downloading wasabi-0.6.0-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting Werkzeug==1.0.0 (from -r /content/ehr-extraction-models/requirements.txt (line 65))\n",
            "  Downloading Werkzeug-1.0.0-py2.py3-none-any.whl.metadata (4.7 kB)\n",
            "Collecting zipp==2.2.0 (from -r /content/ehr-extraction-models/requirements.txt (line 66))\n",
            "  Downloading zipp-2.2.0-py36-none-any.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from image==1.5.28->-r /content/ehr-extraction-models/requirements.txt (line 22)) (11.2.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.7.2->-r /content/ehr-extraction-models/requirements.txt (line 28)) (1.3.2)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.7.2->-r /content/ehr-extraction-models/requirements.txt (line 28)) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.7.2->-r /content/ehr-extraction-models/requirements.txt (line 28)) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.7.2->-r /content/ehr-extraction-models/requirements.txt (line 28)) (24.2)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from preshed==3.0.9->-r /content/ehr-extraction-models/requirements.txt (line 36)) (2.0.11)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from preshed==3.0.9->-r /content/ehr-extraction-models/requirements.txt (line 36)) (1.0.12)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sklearn==0.0->-r /content/ehr-extraction-models/requirements.txt (line 53)) (1.6.1)\n",
            "Collecting blis<0.8.0,>=0.4.0 (from spacy==2.3.9->-r /content/ehr-extraction-models/requirements.txt (line 54))\n",
            "  Downloading blis-0.7.11-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.4 kB)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.11/dist-packages (from tensorboard==2.1.0->-r /content/ehr-extraction-models/requirements.txt (line 57)) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorboard==2.1.0->-r /content/ehr-extraction-models/requirements.txt (line 57)) (1.71.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.11/dist-packages (from tensorboard==2.1.0->-r /content/ehr-extraction-models/requirements.txt (line 57)) (0.45.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from torch==2.0.0->-r /content/ehr-extraction-models/requirements.txt (line 60)) (4.13.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch==2.0.0->-r /content/ehr-extraction-models/requirements.txt (line 60)) (1.13.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.0->-r /content/ehr-extraction-models/requirements.txt (line 60)) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch==2.0.0->-r /content/ehr-extraction-models/requirements.txt (line 60))\n",
            "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu11==11.7.99 (from torch==2.0.0->-r /content/ehr-extraction-models/requirements.txt (line 60))\n",
            "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cuda-cupti-cu11==11.7.101 (from torch==2.0.0->-r /content/ehr-extraction-models/requirements.txt (line 60))\n",
            "  Downloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu11==8.5.0.96 (from torch==2.0.0->-r /content/ehr-extraction-models/requirements.txt (line 60))\n",
            "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu11==11.10.3.66 (from torch==2.0.0->-r /content/ehr-extraction-models/requirements.txt (line 60))\n",
            "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cufft-cu11==10.9.0.58 (from torch==2.0.0->-r /content/ehr-extraction-models/requirements.txt (line 60))\n",
            "  Downloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu11==10.2.10.91 (from torch==2.0.0->-r /content/ehr-extraction-models/requirements.txt (line 60))\n",
            "  Downloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusolver-cu11==11.4.0.1 (from torch==2.0.0->-r /content/ehr-extraction-models/requirements.txt (line 60))\n",
            "  Downloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu11==11.7.4.91 (from torch==2.0.0->-r /content/ehr-extraction-models/requirements.txt (line 60))\n",
            "  Downloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu11==2.14.3 (from torch==2.0.0->-r /content/ehr-extraction-models/requirements.txt (line 60))\n",
            "  Downloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu11==11.7.91 (from torch==2.0.0->-r /content/ehr-extraction-models/requirements.txt (line 60))\n",
            "  Downloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting triton==2.0.0 (from torch==2.0.0->-r /content/ehr-extraction-models/requirements.txt (line 60))\n",
            "  Downloading triton-2.0.0-1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.0 kB)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.30.0->-r /content/ehr-extraction-models/requirements.txt (line 62)) (0.30.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.30.0->-r /content/ehr-extraction-models/requirements.txt (line 62)) (6.0.2)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.30.0->-r /content/ehr-extraction-models/requirements.txt (line 62)) (0.5.3)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.11/dist-packages (from triton==2.0.0->torch==2.0.0->-r /content/ehr-extraction-models/requirements.txt (line 60)) (3.31.6)\n",
            "Collecting lit (from triton==2.0.0->torch==2.0.0->-r /content/ehr-extraction-models/requirements.txt (line 60))\n",
            "  Downloading lit-18.1.8-py3-none-any.whl.metadata (2.5 kB)\n",
            "INFO: pip is looking at multiple versions of contourpy to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting contourpy>=1.0.1 (from matplotlib==3.7.2->-r /content/ehr-extraction-models/requirements.txt (line 28))\n",
            "  Downloading contourpy-1.3.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.4 kB)\n",
            "  Downloading contourpy-1.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.4 kB)\n",
            "  Downloading contourpy-1.2.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.30.0->-r /content/ehr-extraction-models/requirements.txt (line 62)) (2025.3.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.0.0->-r /content/ehr-extraction-models/requirements.txt (line 60)) (3.0.2)\n",
            "INFO: pip is looking at multiple versions of scikit-learn to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting scikit-learn (from sklearn==0.0->-r /content/ehr-extraction-models/requirements.txt (line 53))\n",
            "  Downloading scikit_learn-1.6.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
            "  Downloading scikit_learn-1.5.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "  Downloading scikit_learn-1.5.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "  Downloading scikit_learn-1.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "  Downloading scikit_learn-1.4.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "  Downloading scikit_learn-1.4.1.post1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "  Downloading scikit_learn-1.4.0-1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "INFO: pip is still looking at multiple versions of scikit-learn to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading scikit_learn-1.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "  Downloading scikit_learn-1.3.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "  Downloading scikit_learn-1.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "  Downloading scikit_learn-1.2.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "  Downloading scikit_learn-1.2.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "  Downloading scikit_learn-1.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "  Downloading scikit_learn-1.1.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
            "  Downloading scikit-learn-1.1.2.tar.gz (7.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m90.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mPreparing metadata \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mpyproject.toml\u001b[0m\u001b[1;32m)\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n",
            "Obtaining pytt from git+https://github.com/dmcinerney/pytt.git@4a15322f696fe85a264dd4854fcdb82c9e801c06#egg=pytt\n",
            "  Cloning https://github.com/dmcinerney/pytt.git (to revision 4a15322f696fe85a264dd4854fcdb82c9e801c06) to ./src/pytt\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/dmcinerney/pytt.git /content/src/pytt\n",
            "  Running command git rev-parse -q --verify 'sha^4a15322f696fe85a264dd4854fcdb82c9e801c06'\n",
            "  Running command git fetch -q https://github.com/dmcinerney/pytt.git 4a15322f696fe85a264dd4854fcdb82c9e801c06\n",
            "  Resolved https://github.com/dmcinerney/pytt.git to commit 4a15322f696fe85a264dd4854fcdb82c9e801c06\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Installing collected packages: pytt\n",
            "  Running setup.py develop for pytt\n",
            "Successfully installed pytt-0.1\n",
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m98.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ## 2. Data Preparation\n",
        "# Create a small subset (e.g., 1,000 samples per split) of the preprocessed MIMIC-III .data files.\n",
        "\n",
        "# %%\n",
        "import gzip\n",
        "import json\n",
        "import os\n",
        "\n",
        "# Paths in your Google Drive\n",
        "DATA_DIR = '/content/drive/MyDrive/CS598/DLH'      # @param {type:\"string\"}\n",
        "SUBSET_DIR = '/content/drive/MyDrive/CS598/mimic_subset'       # @param {type:\"string\"}\n",
        "MAX_SAMPLES = 1000                                      # @param {type:\"integer\"}\n",
        "\n",
        "os.makedirs(SUBSET_DIR, exist_ok=True)\n",
        "\n",
        "for split in ['train', 'val', 'test']:\n",
        "    src = os.path.join(DATA_DIR, f\"{split}.data\")\n",
        "    dst = os.path.join(SUBSET_DIR, f\"{split}.data\")\n",
        "    with gzip.open(src, 'rt', encoding='utf-8') as fin, \\\n",
        "         open(dst, 'w', encoding='utf-8') as fout:\n",
        "        for i, line in enumerate(fin):\n",
        "            if i >= MAX_SAMPLES:\n",
        "                break\n",
        "            fout.write(line)\n",
        "\n",
        "print(f\"Created subset files in {SUBSET_DIR}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6YaVccYQZ1Jn",
        "outputId": "6aabd5eb-07ee-42ed-cf88-931ac1125e1b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created subset files in /content/drive/MyDrive/CS598/mimic_subset\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "FILE=/content/src/pytt/pytt/batching/standard_batch_iterator.py\n",
        "\n",
        "# Wrap the queue.put line in IteratorQueueWrapper.__next__ so it won't crash on deepcopy\n",
        "sed -i '/self.iterators.put(copy.deepcopy(self.last_iterator))/c\\\n",
        "        try:\\\n",
        "            self.iterators.put(copy.deepcopy(self.last_iterator))\\\n",
        "        except Exception:\\\n",
        "            pass' \"$FILE\"\n",
        "\n",
        "echo \"Patched IteratorQueueWrapper to skip deepcopy errors.\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hUsqFTbpszS7",
        "outputId": "90eef5ac-2f9b-478f-d85a-ad0123c720ed"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Patched IteratorQueueWrapper to skip deepcopy errors.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Patch SubbatchIndicesIterator to avoid list fallback (use original iterator instead)\n",
        "!sed -i \"s/self.indices_iterator_lookahead = \\[\\]/self.indices_iterator_lookahead = self.indices_iterator/g\" /content/src/pytt/pytt/batching/standard_batch_iterator.py\n"
      ],
      "metadata": {
        "id": "qiXA1ptF3jJ-"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ## 1.x Generate `vocab.txt` for ClinicalBERT\n",
        "# If your ClinicalBERT weights directory is missing `vocab.txt`, you can download it directly from the Hugging Face Hub or extract it via the Transformers library.\n",
        "\n",
        "# %%\n",
        "from transformers import BertTokenizer\n",
        "\n",
        "# Directory where your ClinicalBERT weights are stored\n",
        "dir_path = '/content/drive/MyDrive/CS598/DLH/clinical-bert-weights/ClinicalBERT_pretraining_pytorch_checkpoint'  # update if needed\n",
        "\n",
        "# Option 1: Download directly from the HF Hub\n",
        "!wget -qO {dir_path}/vocab.txt \\\n",
        "    https://huggingface.co/emilyalsentzer/Bio_ClinicalBERT/resolve/main/vocab.txt\n",
        "print('Downloaded vocab.txt to', dir_path)\n",
        "\n",
        "# Now `vocab.txt` should exist alongside your model weights.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LHFwmqp6wLhF",
        "outputId": "bab77673-178b-459e-f41b-a35b57f3da6d"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded vocab.txt to /content/drive/MyDrive/CS598/DLH/clinical-bert-weights/ClinicalBERT_pretraining_pytorch_checkpoint\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "DATA_DIR=\"/content/drive/MyDrive/CS598/mimic_subset\"\n",
        "cd \"$DATA_DIR\"\n",
        "\n",
        "# 1. gzip-compress contents in place, overwriting each foo.data → foo.data.gz\n",
        "gzip -f --no-name *.data\n",
        "\n",
        "# 2. rename foo.data.gz → foo.data so extension is unchanged\n",
        "for f in *.data.gz; do\n",
        "  mv \"$f\" \"${f%.data.gz}.data\"\n",
        "done\n",
        "\n",
        "echo \"Files in $DATA_DIR are now gzipped streams with a .data extension:\"\n",
        "ls -lh *.data | sed 's/^/  /'\n",
        "\n",
        "\n",
        "# Show the first two bytes of one of the files\n",
        "head -c 2 \"$DATA_DIR/val.data\" | xxd\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tCxhpqCGAXEj",
        "outputId": "abea2662-84da-4aa7-ddcf-41a055450710"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files in /content/drive/MyDrive/CS598/mimic_subset are now gzipped streams with a .data extension:\n",
            "  -rw------- 1 root root 25K May  5 22:06 test.data\n",
            "  -rw------- 1 root root 15K May  5 22:06 train.data\n",
            "  -rw------- 1 root root 14K May  5 22:06 val.data\n",
            "00000000: 1f8b                                     ..\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "cat > /content/ehr-extraction-models/processing/dataset.py << 'EOF'\n",
        "import pandas as pd\n",
        "from pytt.preprocessing.raw_dataset import RawDataset\n",
        "\n",
        "class Dataset(RawDataset):\n",
        "    def __init__(self, df):\n",
        "        self.df = df\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        dictionary = super(Dataset, self).__getitem__(i)\n",
        "        # reconstruct the reports DataFrame\n",
        "        dictionary['reports'] = pd.DataFrame(eval(dictionary['reports']))\n",
        "        dictionary['reports']['date'] = pd.to_datetime(dictionary['reports']['date'])\n",
        "        # parse list fields\n",
        "        dictionary['targets'] = eval(dictionary['targets'])\n",
        "        dictionary['labels']  = eval(dictionary['labels'])\n",
        "        return dictionary\n",
        "\n",
        "def init_dataset(filename, limit_rows=None):\n",
        "    \"\"\"\n",
        "    Load the dataset from a gzipped JSON-lines file (the .data format),\n",
        "    optionally limiting the number of rows.\n",
        "    \"\"\"\n",
        "    df = pd.read_json(filename, lines=True, compression='gzip')\n",
        "    if limit_rows is not None:\n",
        "        df = df.head(limit_rows)\n",
        "    return Dataset(df)\n",
        "\n",
        "def split_dataset(filename, split=0.9):\n",
        "    \"\"\"\n",
        "    Load the dataset from a gzipped JSON-lines file, shuffle, and split.\n",
        "    \"\"\"\n",
        "    df = pd.read_json(filename, lines=True, compression='gzip').sample(frac=1)\n",
        "    n = int(round(split * len(df)))\n",
        "    return Dataset(df.iloc[:n]), Dataset(df.iloc[n:])\n",
        "EOF\n",
        "\n",
        "echo \"✅ dataset.py rewritten to read JSON‑lines instead of CSV.\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WWwcS1sigeeQ",
        "outputId": "b4fcc192-7e25-4f0f-a080-c34998fdd307"
      },
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ dataset.py rewritten to read JSON‑lines instead of CSV.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "FILE=/content/ehr-extraction-models/processing/dataset.py\n",
        "\n",
        "# Insert the CSV field‐size bump immediately after the first import\n",
        "sed -i \"1s|^|import sys, csv\\ncsv.field_size_limit(sys.maxsize)\\n|\" \"$FILE\"\n",
        "\n",
        "echo \"✅ Increased CSV field_size_limit in dataset.py\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1q2FgLmDg88D",
        "outputId": "551c1679-2fca-48a4-baac-6e6450a6e849"
      },
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Increased CSV field_size_limit in dataset.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ## 3. Training\n",
        "# Train the sentence-attention model on the small subset.\n",
        "\n",
        "# Path to your ICD code graph pickle (adjust if necessary)\n",
        "# CODE_GRAPH_FILE = \"'/content/synthetic_hierarchy_dict.pkl'\"\n",
        "CODE_GRAPH_FILE = \"'/content/drive/MyDrive/CS598/DLH/code_graph.pkl'\"\n",
        "\n",
        "\n",
        "!mkdir -p /content/supervised\n",
        "!cp /content/drive/MyDrive/CS598/mimic_subset/val.data /content/supervised/supervised.data"
      ],
      "metadata": {
        "id": "IwWvrp68Z77B"
      },
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python {target_repo}/train.py \\\n",
        "  --data_dir {DATA_DIR} \\\n",
        "  --code_graph_file {CODE_GRAPH_FILE} \\\n",
        "  --supervised_data_dir /content/supervised \\\n",
        "  --device cpu \\\n",
        "  code_supervision"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vOax4xq4eSBe",
        "outputId": "8d0f49f4-e78a-4f10-8c06-f072723fcd92"
      },
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-05-06 03:22:25.017766: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1746501745.062664   78810 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1746501745.076120   78810 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-05-06 03:22:25.131340: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ## 4. Evaluation\n",
        "# Evaluate on the validation split and parse the resulting metrics.\n",
        "\n",
        "!python test.py \\\n",
        "  --data_dir /content/drive/MyDrive/mimic_subset \\\n",
        "  sentence_attention checkpoints/sentence_attention_small\n",
        "\n",
        "# Parse scores.txt\n",
        "metrics = {}\n",
        "with open('checkpoints/sentence_attention_small/scores.txt') as f:\n",
        "    for line in f:\n",
        "        if ':' in line:\n",
        "            k, v = line.strip().split(':')\n",
        "            try:\n",
        "                metrics[k] = float(v)\n",
        "            except:\n",
        "                pass\n",
        "print(\"Validation metrics:\", metrics)"
      ],
      "metadata": {
        "id": "fPKfYSbSaAvE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ## 5. Unsupervised Baselines\n",
        "# Implement TF-IDF and Clinical BERT similarity baselines on the same subset.\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import roc_auc_score, average_precision_score\n",
        "\n",
        "# Load validation data\n",
        "val_items = []\n",
        "with open(os.path.join(SUBSET_DIR, 'val.data')) as f:\n",
        "    for i, line in enumerate(f):\n",
        "        if i >= MAX_SAMPLES:\n",
        "            break\n",
        "        val_items.append(json.loads(line))\n",
        "\n",
        "# Example: Extract sentences and query text for TF-IDF\n",
        "sentences = [ ' '.join(item['sentences']) for item in val_items ]\n",
        "queries = [ q_text for q_text in val_items[0]['queries'].values() ]\n",
        "\n",
        "# TF-IDF vectorization\n",
        "vectorizer = TfidfVectorizer(max_features=5000)\n",
        "X = vectorizer.fit_transform(sentences)\n",
        "Q = vectorizer.transform(queries)\n",
        "\n",
        "# Compute cosine similarities and evaluation metrics (placeholder)\n",
        "# ... your code here ...\n",
        "print(\"TF-IDF baseline evaluation placeholder\")"
      ],
      "metadata": {
        "id": "3evXbT_paDvI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ## 6. Interactive Demo\n",
        "# Use `interface.py` to load a trained checkpoint and visualize attention weights for sample queries.\n",
        "\n",
        "from interface import QueryInterface\n",
        "\n",
        "# Initialize interface\n",
        "qi = QueryInterface(\n",
        "    model_type='sentence_attention',\n",
        "    checkpoint_path='checkpoints/sentence_attention_small'\n",
        ")\n",
        "\n",
        "# Example EHR text and query\n",
        "ehr_text = val_items[0]['sentences']\n",
        "query_key = list(val_items[0]['queries'].keys())[0]\n",
        "summary, attention = qi.summarize(ehr_text, query_key)\n",
        "print(\"Query:\", query_key)\n",
        "print(\"Extracted summary:\", summary)\n",
        "print(\"Attention scores:\", attention)\n"
      ],
      "metadata": {
        "id": "49-SivaDaGNh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
