{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fhsu4976/CS598/blob/main/CS598ver2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Colab Notebook for Query-Focused EHR Extractive Summarization\n",
        "# =============================================================\n",
        "# This notebook demonstrates an end-to-end pipeline using a small subset of preprocessed\n",
        "# MIMIC-III .data files to reproduce key experiments from:\n",
        "# \"Query-Focused Extractive Summarization of Electronic Health Records\" (ArXiv:2004.04645).\n"
      ],
      "metadata": {
        "id": "NsVKk4unZkGj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ## 1. Setup\n",
        "# Mount Google Drive, clone the official repo, install dependencies, and download spaCy model.\n",
        "\n",
        "from google.colab import drive\n",
        "import os, sys\n",
        "\n",
        "\n",
        "# Force remount Google Drive to ensure fresh mount\n",
        "drive.mount('/content/drive', force_remount=True)  # Force remount to avoid mountpoint errors\n",
        "\n",
        "# Debug: List top-level of mounted Drive to verify folders\n",
        "print(\"/content/drive contents:\", os.listdir('/content/drive'))\n",
        "print(\"/content/drive/MyDrive contents:\", os.listdir('/content/drive/MyDrive'))\n",
        "\n",
        "# Change to working directory\n",
        "os.chdir('/content')\n",
        "\n",
        "# Clone repository\n",
        "target_repo = '/content/ehr-extraction-models'\n",
        "!rm -rf {target_repo}\n",
        "!git clone https://github.com/dmcinerney/ehr-extraction-models.git {target_repo}\n",
        "\n",
        "# Replace the default requirements.txt with your custom one from Drive\n",
        "# (assumes custom file at /content/drive/MyDrive/CS598/DLH/requirements.txt)\n",
        "custom_req = '/content/drive/MyDrive/CS598/DLH/requirements.txt'\n",
        "!cp {custom_req} {target_repo}/requirements.txt\n",
        "\n",
        "# Install dependencies\n",
        "!pip install -r {target_repo}/requirements.txt\n",
        "!pip install -e git+https://github.com/dmcinerney/pytt.git@4a15322f696fe85a264dd4854fcdb82c9e801c06#egg=pytt\n",
        "\n",
        "# Download spaCy English model\n",
        "!python -m spacy download en_core_web_sm\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "edBIarChZsLe",
        "outputId": "1ac6e129-c3d9-49d2-d47d-fdb53595d170"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive contents: ['Shareddrives', 'MyDrive', '.file-revisions-by-id', '.shortcut-targets-by-id', '.Trash-0']\n",
            "/content/drive/MyDrive contents: ['Chat history for CS598.txt', 'IMG_3594.png', 'Colab Notebooks', 'CS598']\n",
            "Cloning into '/content/ehr-extraction-models'...\n",
            "remote: Enumerating objects: 844, done.\u001b[K\n",
            "remote: Counting objects: 100% (124/124), done.\u001b[K\n",
            "remote: Compressing objects: 100% (64/64), done.\u001b[K\n",
            "remote: Total 844 (delta 68), reused 111 (delta 60), pack-reused 720 (from 1)\u001b[K\n",
            "Receiving objects: 100% (844/844), 3.00 MiB | 17.13 MiB/s, done.\n",
            "Resolving deltas: 100% (492/492), done.\n",
            "Collecting setuptools==68.0.0 (from -r /content/ehr-extraction-models/requirements.txt (line 1))\n",
            "  Using cached setuptools-68.0.0-py3-none-any.whl.metadata (6.4 kB)\n",
            "Collecting asgiref==3.2.3 (from -r /content/ehr-extraction-models/requirements.txt (line 3))\n",
            "  Using cached asgiref-3.2.3-py2.py3-none-any.whl.metadata (8.2 kB)\n",
            "Collecting boto3==1.11.15 (from -r /content/ehr-extraction-models/requirements.txt (line 5))\n",
            "  Using cached boto3-1.11.15-py2.py3-none-any.whl.metadata (4.7 kB)\n",
            "Collecting botocore==1.14.15 (from -r /content/ehr-extraction-models/requirements.txt (line 6))\n",
            "  Using cached botocore-1.14.15-py2.py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting cachetools==4.0.0 (from -r /content/ehr-extraction-models/requirements.txt (line 7))\n",
            "  Using cached cachetools-4.0.0-py3-none-any.whl.metadata (4.4 kB)\n",
            "Collecting catalogue==1.0.0 (from -r /content/ehr-extraction-models/requirements.txt (line 8))\n",
            "  Using cached catalogue-1.0.0-py2.py3-none-any.whl.metadata (13 kB)\n",
            "Collecting certifi==2019.11.28 (from -r /content/ehr-extraction-models/requirements.txt (line 9))\n",
            "  Using cached certifi-2019.11.28-py2.py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting chardet==3.0.4 (from -r /content/ehr-extraction-models/requirements.txt (line 10))\n",
            "  Using cached chardet-3.0.4-py2.py3-none-any.whl.metadata (3.2 kB)\n",
            "Collecting Click==7.0 (from -r /content/ehr-extraction-models/requirements.txt (line 11))\n",
            "  Using cached Click-7.0-py2.py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting cycler==0.10.0 (from -r /content/ehr-extraction-models/requirements.txt (line 12))\n",
            "  Using cached cycler-0.10.0-py2.py3-none-any.whl.metadata (722 bytes)\n",
            "Collecting decorator==4.4.1 (from -r /content/ehr-extraction-models/requirements.txt (line 14))\n",
            "  Using cached decorator-4.4.1-py2.py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting Django==3.0.3 (from -r /content/ehr-extraction-models/requirements.txt (line 15))\n",
            "  Using cached Django-3.0.3-py3-none-any.whl.metadata (3.6 kB)\n",
            "Collecting docutils==0.15.2 (from -r /content/ehr-extraction-models/requirements.txt (line 16))\n",
            "  Using cached docutils-0.15.2-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting filelock==3.0.12 (from -r /content/ehr-extraction-models/requirements.txt (line 17))\n",
            "  Using cached filelock-3.0.12-py3-none-any.whl.metadata (4.3 kB)\n",
            "Collecting google-auth==1.11.0 (from -r /content/ehr-extraction-models/requirements.txt (line 18))\n",
            "  Using cached google_auth-1.11.0-py2.py3-none-any.whl.metadata (3.1 kB)\n",
            "Collecting google-auth-oauthlib==0.4.1 (from -r /content/ehr-extraction-models/requirements.txt (line 19))\n",
            "  Using cached google_auth_oauthlib-0.4.1-py2.py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting idna==2.8 (from -r /content/ehr-extraction-models/requirements.txt (line 21))\n",
            "  Using cached idna-2.8-py2.py3-none-any.whl.metadata (8.9 kB)\n",
            "Collecting image==1.5.28 (from -r /content/ehr-extraction-models/requirements.txt (line 22))\n",
            "  Using cached image-1.5.28.tar.gz (15 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting importlib-metadata==1.5.0 (from -r /content/ehr-extraction-models/requirements.txt (line 23))\n",
            "  Using cached importlib_metadata-1.5.0-py2.py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting jmespath==0.9.4 (from -r /content/ehr-extraction-models/requirements.txt (line 24))\n",
            "  Using cached jmespath-0.9.4-py2.py3-none-any.whl.metadata (7.9 kB)\n",
            "Collecting joblib==0.14.1 (from -r /content/ehr-extraction-models/requirements.txt (line 25))\n",
            "  Using cached joblib-0.14.1-py2.py3-none-any.whl.metadata (4.5 kB)\n",
            "Collecting Markdown==3.2 (from -r /content/ehr-extraction-models/requirements.txt (line 27))\n",
            "  Using cached Markdown-3.2-py2.py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting matplotlib==3.7.2 (from -r /content/ehr-extraction-models/requirements.txt (line 28))\n",
            "  Using cached matplotlib-3.7.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
            "Collecting networkx==2.4 (from -r /content/ehr-extraction-models/requirements.txt (line 30))\n",
            "  Using cached networkx-2.4-py3-none-any.whl.metadata (4.9 kB)\n",
            "Collecting numpy==1.22.4 (from -r /content/ehr-extraction-models/requirements.txt (line 31))\n",
            "  Using cached numpy-1.22.4.zip (11.5 MB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting oauthlib==3.1.0 (from -r /content/ehr-extraction-models/requirements.txt (line 32))\n",
            "  Using cached oauthlib-3.1.0-py2.py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting plac==1.1.3 (from -r /content/ehr-extraction-models/requirements.txt (line 35))\n",
            "  Using cached plac-1.1.3-py2.py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: preshed==3.0.9 in /usr/local/lib/python3.11/dist-packages (from -r /content/ehr-extraction-models/requirements.txt (line 36)) (3.0.9)\n",
            "Collecting protobuf==3.11.3 (from -r /content/ehr-extraction-models/requirements.txt (line 37))\n",
            "  Using cached protobuf-3.11.3-py2.py3-none-any.whl.metadata (884 bytes)\n",
            "Collecting pyasn1==0.4.8 (from -r /content/ehr-extraction-models/requirements.txt (line 38))\n",
            "  Using cached pyasn1-0.4.8-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting pyasn1-modules==0.2.8 (from -r /content/ehr-extraction-models/requirements.txt (line 39))\n",
            "  Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting pyparsing==2.4.6 (from -r /content/ehr-extraction-models/requirements.txt (line 40))\n",
            "  Using cached pyparsing-2.4.6-py2.py3-none-any.whl.metadata (3.6 kB)\n",
            "Collecting python-dateutil==2.8.1 (from -r /content/ehr-extraction-models/requirements.txt (line 41))\n",
            "  Using cached python_dateutil-2.8.1-py2.py3-none-any.whl.metadata (8.0 kB)\n",
            "Collecting pytz==2019.3 (from -r /content/ehr-extraction-models/requirements.txt (line 42))\n",
            "  Using cached pytz-2019.3-py2.py3-none-any.whl.metadata (20 kB)\n",
            "Collecting regex==2020.1.8 (from -r /content/ehr-extraction-models/requirements.txt (line 43))\n",
            "  Using cached regex-2020.1.8.tar.gz (681 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting requests==2.22.0 (from -r /content/ehr-extraction-models/requirements.txt (line 44))\n",
            "  Using cached requests-2.22.0-py2.py3-none-any.whl.metadata (5.5 kB)\n",
            "Collecting requests-oauthlib==1.3.0 (from -r /content/ehr-extraction-models/requirements.txt (line 45))\n",
            "  Using cached requests_oauthlib-1.3.0-py2.py3-none-any.whl.metadata (9.6 kB)\n",
            "Collecting rsa==4.0 (from -r /content/ehr-extraction-models/requirements.txt (line 46))\n",
            "  Using cached rsa-4.0-py2.py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting s3transfer==0.3.3 (from -r /content/ehr-extraction-models/requirements.txt (line 47))\n",
            "  Using cached s3transfer-0.3.3-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting sacremoses==0.0.38 (from -r /content/ehr-extraction-models/requirements.txt (line 48))\n",
            "  Using cached sacremoses-0.0.38.tar.gz (860 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting scipy==1.13.0 (from -r /content/ehr-extraction-models/requirements.txt (line 50))\n",
            "  Using cached scipy-1.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "Collecting sentencepiece==0.1.99 (from -r /content/ehr-extraction-models/requirements.txt (line 51))\n",
            "  Using cached sentencepiece-0.1.99-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
            "Collecting six==1.14.0 (from -r /content/ehr-extraction-models/requirements.txt (line 52))\n",
            "  Using cached six-1.14.0-py2.py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting sklearn==0.0 (from -r /content/ehr-extraction-models/requirements.txt (line 53))\n",
            "  Using cached sklearn-0.0.tar.gz (1.1 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting spacy==2.3.9 (from -r /content/ehr-extraction-models/requirements.txt (line 54))\n",
            "  Using cached spacy-2.3.9-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (15 kB)\n",
            "Collecting sqlparse==0.3.0 (from -r /content/ehr-extraction-models/requirements.txt (line 55))\n",
            "  Using cached sqlparse-0.3.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting srsly==1.0.7 (from -r /content/ehr-extraction-models/requirements.txt (line 56))\n",
            "  Using cached srsly-1.0.7-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Collecting tensorboard==2.1.0 (from -r /content/ehr-extraction-models/requirements.txt (line 57))\n",
            "  Using cached tensorboard-2.1.0-py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting thinc==7.4.6 (from -r /content/ehr-extraction-models/requirements.txt (line 58))\n",
            "  Using cached thinc-7.4.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (23 kB)\n",
            "Collecting tokenizers==0.13.3 (from -r /content/ehr-extraction-models/requirements.txt (line 59))\n",
            "  Using cached tokenizers-0.13.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Collecting torch==2.0.0 (from -r /content/ehr-extraction-models/requirements.txt (line 60))\n",
            "  Using cached torch-2.0.0-cp311-cp311-manylinux1_x86_64.whl.metadata (24 kB)\n",
            "Collecting tqdm==4.42.1 (from -r /content/ehr-extraction-models/requirements.txt (line 61))\n",
            "  Using cached tqdm-4.42.1-py2.py3-none-any.whl.metadata (49 kB)\n",
            "Collecting transformers==4.30.0 (from -r /content/ehr-extraction-models/requirements.txt (line 62))\n",
            "  Using cached transformers-4.30.0-py3-none-any.whl.metadata (113 kB)\n",
            "Collecting urllib3==1.25.8 (from -r /content/ehr-extraction-models/requirements.txt (line 63))\n",
            "  Using cached urllib3-1.25.8-py2.py3-none-any.whl.metadata (38 kB)\n",
            "Collecting wasabi==0.6.0 (from -r /content/ehr-extraction-models/requirements.txt (line 64))\n",
            "  Using cached wasabi-0.6.0-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting Werkzeug==1.0.0 (from -r /content/ehr-extraction-models/requirements.txt (line 65))\n",
            "  Using cached Werkzeug-1.0.0-py2.py3-none-any.whl.metadata (4.7 kB)\n",
            "Collecting zipp==2.2.0 (from -r /content/ehr-extraction-models/requirements.txt (line 66))\n",
            "  Using cached zipp-2.2.0-py36-none-any.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from image==1.5.28->-r /content/ehr-extraction-models/requirements.txt (line 22)) (11.2.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.7.2->-r /content/ehr-extraction-models/requirements.txt (line 28)) (1.3.2)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.7.2->-r /content/ehr-extraction-models/requirements.txt (line 28)) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.7.2->-r /content/ehr-extraction-models/requirements.txt (line 28)) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.7.2->-r /content/ehr-extraction-models/requirements.txt (line 28)) (24.2)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from preshed==3.0.9->-r /content/ehr-extraction-models/requirements.txt (line 36)) (2.0.11)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from preshed==3.0.9->-r /content/ehr-extraction-models/requirements.txt (line 36)) (1.0.12)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sklearn==0.0->-r /content/ehr-extraction-models/requirements.txt (line 53)) (1.6.1)\n",
            "Collecting blis<0.8.0,>=0.4.0 (from spacy==2.3.9->-r /content/ehr-extraction-models/requirements.txt (line 54))\n",
            "  Using cached blis-0.7.11-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.4 kB)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.11/dist-packages (from tensorboard==2.1.0->-r /content/ehr-extraction-models/requirements.txt (line 57)) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorboard==2.1.0->-r /content/ehr-extraction-models/requirements.txt (line 57)) (1.71.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.11/dist-packages (from tensorboard==2.1.0->-r /content/ehr-extraction-models/requirements.txt (line 57)) (0.45.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from torch==2.0.0->-r /content/ehr-extraction-models/requirements.txt (line 60)) (4.13.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch==2.0.0->-r /content/ehr-extraction-models/requirements.txt (line 60)) (1.13.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.0->-r /content/ehr-extraction-models/requirements.txt (line 60)) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch==2.0.0->-r /content/ehr-extraction-models/requirements.txt (line 60))\n",
            "  Using cached nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu11==11.7.99 (from torch==2.0.0->-r /content/ehr-extraction-models/requirements.txt (line 60))\n",
            "  Using cached nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cuda-cupti-cu11==11.7.101 (from torch==2.0.0->-r /content/ehr-extraction-models/requirements.txt (line 60))\n",
            "  Using cached nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu11==8.5.0.96 (from torch==2.0.0->-r /content/ehr-extraction-models/requirements.txt (line 60))\n",
            "  Using cached nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu11==11.10.3.66 (from torch==2.0.0->-r /content/ehr-extraction-models/requirements.txt (line 60))\n",
            "  Using cached nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cufft-cu11==10.9.0.58 (from torch==2.0.0->-r /content/ehr-extraction-models/requirements.txt (line 60))\n",
            "  Using cached nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu11==10.2.10.91 (from torch==2.0.0->-r /content/ehr-extraction-models/requirements.txt (line 60))\n",
            "  Using cached nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusolver-cu11==11.4.0.1 (from torch==2.0.0->-r /content/ehr-extraction-models/requirements.txt (line 60))\n",
            "  Using cached nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu11==11.7.4.91 (from torch==2.0.0->-r /content/ehr-extraction-models/requirements.txt (line 60))\n",
            "  Using cached nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu11==2.14.3 (from torch==2.0.0->-r /content/ehr-extraction-models/requirements.txt (line 60))\n",
            "  Using cached nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu11==11.7.91 (from torch==2.0.0->-r /content/ehr-extraction-models/requirements.txt (line 60))\n",
            "  Using cached nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting triton==2.0.0 (from torch==2.0.0->-r /content/ehr-extraction-models/requirements.txt (line 60))\n",
            "  Using cached triton-2.0.0-1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.0 kB)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.30.0->-r /content/ehr-extraction-models/requirements.txt (line 62)) (0.30.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.30.0->-r /content/ehr-extraction-models/requirements.txt (line 62)) (6.0.2)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.30.0->-r /content/ehr-extraction-models/requirements.txt (line 62)) (0.5.3)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.11/dist-packages (from triton==2.0.0->torch==2.0.0->-r /content/ehr-extraction-models/requirements.txt (line 60)) (3.31.6)\n",
            "Collecting lit (from triton==2.0.0->torch==2.0.0->-r /content/ehr-extraction-models/requirements.txt (line 60))\n",
            "  Using cached lit-18.1.8-py3-none-any.whl.metadata (2.5 kB)\n",
            "INFO: pip is looking at multiple versions of contourpy to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting contourpy>=1.0.1 (from matplotlib==3.7.2->-r /content/ehr-extraction-models/requirements.txt (line 28))\n",
            "  Using cached contourpy-1.3.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.4 kB)\n",
            "  Using cached contourpy-1.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.4 kB)\n",
            "  Using cached contourpy-1.2.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.30.0->-r /content/ehr-extraction-models/requirements.txt (line 62)) (2025.3.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.0.0->-r /content/ehr-extraction-models/requirements.txt (line 60)) (3.0.2)\n",
            "INFO: pip is looking at multiple versions of scikit-learn to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting scikit-learn (from sklearn==0.0->-r /content/ehr-extraction-models/requirements.txt (line 53))\n",
            "  Using cached scikit_learn-1.6.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
            "  Using cached scikit_learn-1.5.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "  Using cached scikit_learn-1.5.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "  Using cached scikit_learn-1.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "  Using cached scikit_learn-1.4.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "  Using cached scikit_learn-1.4.1.post1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "  Using cached scikit_learn-1.4.0-1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "INFO: pip is still looking at multiple versions of scikit-learn to determine which version is compatible with other requirements. This could take a while.\n",
            "  Using cached scikit_learn-1.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "  Using cached scikit_learn-1.3.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "  Using cached scikit_learn-1.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "  Using cached scikit_learn-1.2.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "  Using cached scikit_learn-1.2.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "  Using cached scikit_learn-1.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "  Using cached scikit_learn-1.1.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
            "  Using cached scikit-learn-1.1.2.tar.gz (7.0 MB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mPreparing metadata \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mpyproject.toml\u001b[0m\u001b[1;32m)\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n",
            "Obtaining pytt from git+https://github.com/dmcinerney/pytt.git@4a15322f696fe85a264dd4854fcdb82c9e801c06#egg=pytt\n",
            "  Skipping because already up-to-date.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Installing collected packages: pytt\n",
            "  Attempting uninstall: pytt\n",
            "    Found existing installation: pytt 0.1\n",
            "    Uninstalling pytt-0.1:\n",
            "      Successfully uninstalled pytt-0.1\n",
            "  Running setup.py develop for pytt\n",
            "Successfully installed pytt-0.1\n",
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m54.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ## 2. Data Preparation\n",
        "# Create a small subset (e.g., 1,000 samples per split) of the preprocessed MIMIC-III .data files.\n",
        "\n",
        "# %%\n",
        "import gzip\n",
        "import json\n",
        "import os\n",
        "\n",
        "# Paths in your Google Drive\n",
        "DATA_DIR = '/content/drive/MyDrive/CS598/DLH'      # @param {type:\"string\"}\n",
        "SUBSET_DIR = '/content/drive/MyDrive/CS598/mimic_subset'       # @param {type:\"string\"}\n",
        "MAX_SAMPLES = 100                                      # @param {type:\"integer\"}\n",
        "\n",
        "os.makedirs(SUBSET_DIR, exist_ok=True)\n",
        "\n",
        "for split in ['train', 'val', 'test']:\n",
        "    src = os.path.join(DATA_DIR, f\"{split}.data\")\n",
        "    dst = os.path.join(SUBSET_DIR, f\"{split}.data\")\n",
        "    with gzip.open(src, 'rt', encoding='utf-8') as fin, \\\n",
        "         open(dst, 'w', encoding='utf-8') as fout:\n",
        "        for i, line in enumerate(fin):\n",
        "            if i >= MAX_SAMPLES:\n",
        "                break\n",
        "            fout.write(line)\n",
        "\n",
        "print(f\"Created subset files in {SUBSET_DIR}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6YaVccYQZ1Jn",
        "outputId": "606215d8-1737-45b8-de0e-c284cc86b94e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created subset files in /content/drive/MyDrive/CS598/mimic_subset\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# not needed??\n",
        "\n",
        "# %%bash\n",
        "# FILE=/content/src/pytt/pytt/batching/standard_batch_iterator.py\n",
        "\n",
        "# # Wrap the queue.put line in IteratorQueueWrapper.__next__ so it won't crash on deepcopy\n",
        "# sed -i '/self\\.iterators\\.put(copy.deepcopy(self\\.last_iterator))/c\\\n",
        "#         try:\\\n",
        "#             self.iterators.put(copy.deepcopy(self.last_iterator))\\\n",
        "#         except Exception:\\\n",
        "#             pass' \"$FILE\"\n",
        "\n",
        "# echo \"Patched IteratorQueueWrapper to skip deepcopy errors.\"\n"
      ],
      "metadata": {
        "id": "hUsqFTbpszS7"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# used with above cell\n",
        "# # Patch SubbatchIndicesIterator to avoid list fallback (use original iterator instead)\n",
        "# !sed -i \"s/self.indices_iterator_lookahead = \\[\\]/self.indices_iterator_lookahead = self.indices_iterator/g\" /content/src/pytt/pytt/batching/standard_batch_iterator.py\n"
      ],
      "metadata": {
        "id": "qiXA1ptF3jJ-"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ## 1.x Generate `vocab.txt` for ClinicalBERT\n",
        "# If your ClinicalBERT weights directory is missing `vocab.txt`, you can download it directly from the Hugging Face Hub or extract it via the Transformers library.\n",
        "\n",
        "# %%\n",
        "from transformers import BertTokenizer\n",
        "\n",
        "# Directory where your ClinicalBERT weights are stored\n",
        "dir_path = '/content/drive/MyDrive/CS598/DLH/clinical-bert-weights/ClinicalBERT_pretraining_pytorch_checkpoint'  # update if needed\n",
        "\n",
        "# Option 1: Download directly from the HF Hub\n",
        "!wget -qO {dir_path}/vocab.txt \\\n",
        "    https://huggingface.co/emilyalsentzer/Bio_ClinicalBERT/resolve/main/vocab.txt\n",
        "print('Downloaded vocab.txt to', dir_path)\n",
        "\n",
        "# Now `vocab.txt` should exist alongside your model weights.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LHFwmqp6wLhF",
        "outputId": "6b03f11e-ce64-414a-8782-61d6fbefdb8c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded vocab.txt to /content/drive/MyDrive/CS598/DLH/clinical-bert-weights/ClinicalBERT_pretraining_pytorch_checkpoint\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "DATA_DIR=\"/content/drive/MyDrive/CS598/mimic_subset\"\n",
        "cd \"$DATA_DIR\"\n",
        "\n",
        "# 1. gzip-compress contents in place, overwriting each foo.data → foo.data.gz\n",
        "gzip -f --no-name *.data\n",
        "\n",
        "# 2. rename foo.data.gz → foo.data so extension is unchanged\n",
        "for f in *.data.gz; do\n",
        "  mv \"$f\" \"${f%.data.gz}.data\"\n",
        "done\n",
        "\n",
        "echo \"Files in $DATA_DIR are now gzipped streams with a .data extension:\"\n",
        "ls -lh *.data | sed 's/^/  /'\n",
        "\n",
        "\n",
        "# Show the first two bytes of one of the files\n",
        "head -c 2 \"$DATA_DIR/val.data\" | xxd\n",
        "head -c 2 \"$DATA_DIR/train.data\" | xxd\n",
        "head -c 2 \"$DATA_DIR/test.data\" | xxd\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tCxhpqCGAXEj",
        "outputId": "47984768-ccec-4a18-de91-dd6b721ce2b4"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files in /content/drive/MyDrive/CS598/mimic_subset are now gzipped streams with a .data extension:\n",
            "  -rw------- 1 root root 1.7K May  7 00:15 test.data\n",
            "  -rw------- 1 root root 2.6K May  7 00:15 train.data\n",
            "  -rw------- 1 root root 2.6K May  7 00:15 val.data\n",
            "00000000: 1f8b                                     ..\n",
            "00000000: 1f8b                                     ..\n",
            "00000000: 1f8b                                     ..\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# not needed? testing with original dataset.py\n",
        "\n",
        "# %%bash\n",
        "# cat > /content/ehr-extraction-models/processing/dataset.py << 'EOF'\n",
        "# import pandas as pd\n",
        "# from pytt.preprocessing.raw_dataset import RawDataset\n",
        "\n",
        "# class Dataset(RawDataset):\n",
        "#     def __init__(self, df):\n",
        "#         self.df = df\n",
        "\n",
        "#     def __getitem__(self, i):\n",
        "#         dictionary = super(Dataset, self).__getitem__(i)\n",
        "#         # reconstruct the reports DataFrame\n",
        "#         dictionary['reports'] = pd.DataFrame(eval(dictionary['reports']))\n",
        "#         dictionary['reports']['date'] = pd.to_datetime(dictionary['reports']['date'])\n",
        "#         # parse list fields\n",
        "#         dictionary['targets'] = eval(dictionary['targets'])\n",
        "#         dictionary['labels']  = eval(dictionary['labels'])\n",
        "#         return dictionary\n",
        "\n",
        "# def init_dataset(filename, limit_rows=None):\n",
        "#     \"\"\"\n",
        "#     Load the dataset from a gzipped JSON-lines file (the .data format),\n",
        "#     optionally limiting the number of rows.\n",
        "#     \"\"\"\n",
        "#     df = pd.read_json(filename, lines=True, compression='gzip')\n",
        "#     if limit_rows is not None:\n",
        "#         df = df.head(limit_rows)\n",
        "#     return Dataset(df)\n",
        "\n",
        "# def split_dataset(filename, split=0.9):\n",
        "#     \"\"\"\n",
        "#     Load the dataset from a gzipped JSON-lines file, shuffle, and split.\n",
        "#     \"\"\"\n",
        "#     df = pd.read_json(filename, lines=True, compression='gzip').sample(frac=1)\n",
        "#     n = int(round(split * len(df)))\n",
        "#     return Dataset(df.iloc[:n]), Dataset(df.iloc[n:])\n",
        "# EOF\n",
        "\n",
        "# echo \"✅ dataset.py rewritten to read JSON‑lines instead of CSV.\"\n"
      ],
      "metadata": {
        "id": "WWwcS1sigeeQ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %%bash\n",
        "# FILE=/content/ehr-extraction-models/processing/dataset.py\n",
        "\n",
        "# # Insert the CSV field‐size bump immediately after the first import\n",
        "# sed -i \"1s|^|import sys, csv\\ncsv.field_size_limit(sys.maxsize)\\n|\" \"$FILE\"\n",
        "\n",
        "# echo \"✅ Increased CSV field_size_limit in dataset.py\"\n"
      ],
      "metadata": {
        "id": "1q2FgLmDg88D"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "FILE=/content/ehr-extraction-models/processing/tokenizer.py\n",
        "LOCAL_DIR=/content/drive/MyDrive/CS598/DLH/clinical-bert-weights/ClinicalBERT_pretraining_pytorch_checkpoint\n",
        "\n",
        "# 1) Ensure os is imported\n",
        "grep -q \"^import os\" \"$FILE\" || sed -i \"1s|^|import os\\n|\" \"$FILE\"\n",
        "\n",
        "# 2) Replace the from_pretrained line with a direct vocab_file load\n",
        "sed -i \"s|self.tokenizer = BertTokenizer.from_pretrained(p.pretrained_model.*|self.tokenizer = BertTokenizer(vocab_file=os.path.join('$LOCAL_DIR','vocab.txt'), do_lower_case=False)|\" \"$FILE\"\n",
        "\n",
        "echo \"✅ tokenizer.py now hardcodes local vocab.txt path.\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xPQ0NCAg988s",
        "outputId": "55abb0b2-1ea3-4e59-b342-84e280acd451"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ tokenizer.py now hardcodes local vocab.txt path.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pytt, os\n",
        "\n",
        "# Where is the pytt package __init__.py?\n",
        "print(\"pytt __file__:\", pytt.__file__)\n",
        "\n",
        "# Build the path to the iterator file\n",
        "iterator_file = os.path.join(\n",
        "    os.path.dirname(pytt.__file__),\n",
        "    \"batching\",\n",
        "    \"standard_batch_iterator.py\"\n",
        ")\n",
        "print(\"Iterator at:\", iterator_file)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263
        },
        "id": "kym6SmU9Ram_",
        "outputId": "afdb0974-1b69-4074-da78-539fecea2502"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pytt __file__: None\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "expected str, bytes or os.PathLike object, not NoneType",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-2b0686a71bbf>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Build the path to the iterator file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m iterator_file = os.path.join(\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpytt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__file__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0;34m\"batching\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;34m\"standard_batch_iterator.py\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/posixpath.py\u001b[0m in \u001b[0;36mdirname\u001b[0;34m(p)\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: expected str, bytes or os.PathLike object, not NoneType"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %%bash\n",
        "# FILE=/content/src/pytt/pytt/batching/standard_batch_iterator.py\n",
        "\n",
        "# # Patch subbatch generator deepcopy to skip non-pickleable iterators\n",
        "# sed -i -E '/^[[:space:]]*copy\\.deepcopy\\(self\\.subbatch_indices_iterator\\)/c\\\n",
        "#     try:\\\n",
        "#          self.subbatch_indices_iterator = copy.deepcopy(self.subbatch_indices_iterator)\\\n",
        "#     except Exception:\\\n",
        "#          self.subbatch_indices_iterator = None' \"$FILE\"\n",
        "\n",
        "# echo \"✅ Patched StandardBatchIterator to skip subbatch generator deepcopy errors.\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MJgKvFRHLdkY",
        "outputId": "5c42bac3-47e4-4757-c84f-f9e54bed87ef"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Patched StandardBatchIterator to skip subbatch generator deepcopy errors.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "FILE=/content/src/pytt/pytt/batching/standard_batch_iterator.py\n",
        "\n",
        "# Insert num_workers=0 immediately after the first positional dataset argument\n",
        "sed -i -E \"s|(StandardBatchIterator\\(\\s*DataLoader\\(\\s*([^)]+))|\\1, num_workers=0|\" \"$FILE\"\n",
        "\n",
        "echo \"✅ DataLoader patched to set num_workers=0 in the correct position.\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UZGHzXV6O6x-",
        "outputId": "e44c190e-51ec-4aaa-a66c-15559814ded6"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ DataLoader patched to set num_workers=0 in the correct position.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %%bash\n",
        "# FILE=/content/src/pytt/pytt/batching/standard_batch_iterator.py\n",
        "\n",
        "# # Replace the deepcopy put with a safe non-deepcopy wrapper\n",
        "# sed -i \"/self.iterators.put(copy.deepcopy(self.last_iterator))/c\\\n",
        "#             try:\\\n",
        "#                 self.iterators.put(self.last_iterator)\\\n",
        "#             except Exception:\\\n",
        "#                 pass\" \"$FILE\"\n",
        "\n",
        "# echo \"✅ Updated IteratorQueueWrapper to skip deepcopy and use direct iterator.\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IXgPqF8SP88w",
        "outputId": "ed86a409-39d3-4ecc-8f73-fdcca6dc3d3e"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Updated IteratorQueueWrapper to skip deepcopy and use direct iterator.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ## 3. Training\n",
        "# Train the sentence-attention model on the small subset.\n",
        "\n",
        "# Path to your ICD code graph pickle (adjust if necessary)\n",
        "CODE_GRAPH_FILE = '/content/drive/MyDrive/CS598/DLH/code_graph.pkl'\n",
        "\n",
        "\n",
        "!mkdir -p /content/supervised\n",
        "!cp /content/drive/MyDrive/CS598/mimic_subset/val.data /content/supervised/supervised.data\n",
        "!cp {SUBSET_DIR}/train.data /content/supervised/train.data"
      ],
      "metadata": {
        "id": "IwWvrp68Z77B"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python {target_repo}/train.py \\\n",
        "  --data_dir {DATA_DIR} \\\n",
        "  --code_graph_file {CODE_GRAPH_FILE} \\\n",
        "  --supervised_data_dir /content/supervised \\\n",
        "  --device cpu \\\n",
        "  code_supervision"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vOax4xq4eSBe",
        "outputId": "98eddc9b-129d-4bd2-d108-ad3038cbc995"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-05-07 01:00:32.385074: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1746579632.427044   33287 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1746579632.439013   33287 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-05-07 01:00:32.477052: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/ehr-extraction-models/train.py\", line 158, in <module>\n",
            "    raise e\n",
            "  File \"/content/ehr-extraction-models/train.py\", line 145, in <module>\n",
            "    main(args.model_type, train_file, hierarchy, counts_file, val_file=val_file,\n",
            "  File \"/content/ehr-extraction-models/train.py\", line 79, in main\n",
            "    batch_iterator = batcher.batch_iterator(train_dataset, indices_iterator, subbatches=subbatches, num_workers=num_workers)\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/src/pytt/pytt/batching/standard_batcher.py\", line 49, in batch_iterator\n",
            "    return StandardBatchIterator(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/src/pytt/pytt/batching/standard_batch_iterator.py\", line 60, in __init__\n",
            "    self.dataloaderiter = iter(DataLoader(\n",
            "                          ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 491, in __iter__\n",
            "    return self._get_iterator()\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 422, in _get_iterator\n",
            "    return _MultiProcessingDataLoaderIter(self)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1199, in __init__\n",
            "    self._reset(loader, first_iter=True)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1236, in _reset\n",
            "    self._try_put_index()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1486, in _try_put_index\n",
            "    index = self._next_index()\n",
            "            ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 698, in _next_index\n",
            "    return next(self._sampler_iter)  # may raise StopIteration\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/src/pytt/pytt/batching/standard_batch_iterator.py\", line 158, in __next__\n",
            "    self.iterators.put(copy.deepcopy(self.last_iterator))\n",
            "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/copy.py\", line 172, in deepcopy\n",
            "    y = _reconstruct(x, memo, *rv)\n",
            "        ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/copy.py\", line 271, in _reconstruct\n",
            "    state = deepcopy(state, memo)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/copy.py\", line 146, in deepcopy\n",
            "    y = copier(x, memo)\n",
            "        ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/copy.py\", line 231, in _deepcopy_dict\n",
            "    y[deepcopy(key, memo)] = deepcopy(value, memo)\n",
            "                             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/copy.py\", line 161, in deepcopy\n",
            "    rv = reductor(4)\n",
            "         ^^^^^^^^^^^\n",
            "TypeError: cannot pickle 'generator' object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ## 4. Evaluation\n",
        "# Evaluate on the validation split and parse the resulting metrics.\n",
        "\n",
        "!python test.py \\\n",
        "  --data_dir /content/drive/MyDrive/CS598/mimic_subset \\\n",
        "  code_supervision\n",
        "\n",
        "# Parse scores.txt\n",
        "metrics = {}\n",
        "with open('checkpoints/sentence_attention_small/scores.txt') as f:\n",
        "    for line in f:\n",
        "        if ':' in line:\n",
        "            k, v = line.strip().split(':')\n",
        "            try:\n",
        "                metrics[k] = float(v)\n",
        "            except:\n",
        "                pass\n",
        "print(\"Validation metrics:\", metrics)"
      ],
      "metadata": {
        "id": "fPKfYSbSaAvE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "outputId": "b80eb1f6-3e51-4dc8-92cb-a03ef6714acb"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "python3: can't open file '/content/test.py': [Errno 2] No such file or directory\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'checkpoints/sentence_attention_small/scores.txt'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-cd9e1ba650f3>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Parse scores.txt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'checkpoints/sentence_attention_small/scores.txt'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m':'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'checkpoints/sentence_attention_small/scores.txt'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ## 5. Unsupervised Baselines\n",
        "# Implement TF-IDF and Clinical BERT similarity baselines on the same subset.\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import roc_auc_score, average_precision_score\n",
        "\n",
        "# Load validation data\n",
        "val_items = []\n",
        "with open(os.path.join(SUBSET_DIR, 'val.data')) as f:\n",
        "    for i, line in enumerate(f):\n",
        "        if i >= MAX_SAMPLES:\n",
        "            break\n",
        "        val_items.append(json.loads(line))\n",
        "\n",
        "# Example: Extract sentences and query text for TF-IDF\n",
        "sentences = [ ' '.join(item['sentences']) for item in val_items ]\n",
        "queries = [ q_text for q_text in val_items[0]['queries'].values() ]\n",
        "\n",
        "# TF-IDF vectorization\n",
        "vectorizer = TfidfVectorizer(max_features=5000)\n",
        "X = vectorizer.fit_transform(sentences)\n",
        "Q = vectorizer.transform(queries)\n",
        "\n",
        "# Compute cosine similarities and evaluation metrics (placeholder)\n",
        "# ... your code here ...\n",
        "print(\"TF-IDF baseline evaluation placeholder\")"
      ],
      "metadata": {
        "id": "3evXbT_paDvI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ## 6. Interactive Demo\n",
        "# Use `interface.py` to load a trained checkpoint and visualize attention weights for sample queries.\n",
        "\n",
        "from interface import QueryInterface\n",
        "\n",
        "# Initialize interface\n",
        "qi = QueryInterface(\n",
        "    model_type='sentence_attention',\n",
        "    checkpoint_path='checkpoints/sentence_attention_small'\n",
        ")\n",
        "\n",
        "# Example EHR text and query\n",
        "ehr_text = val_items[0]['sentences']\n",
        "query_key = list(val_items[0]['queries'].keys())[0]\n",
        "summary, attention = qi.summarize(ehr_text, query_key)\n",
        "print(\"Query:\", query_key)\n",
        "print(\"Extracted summary:\", summary)\n",
        "print(\"Attention scores:\", attention)\n"
      ],
      "metadata": {
        "id": "49-SivaDaGNh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}